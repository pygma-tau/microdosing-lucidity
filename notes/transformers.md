---
title: Transformers
---

To put somewhere: intuitions for **activation space** should be similar to those for phase space. GPT-2-small can be considered to have 12 x 768 neurons (check this!!), and each of these neurons is an element of this 'activation space'. Perhaps it's also good to completely reconstruct an NN architecture. 

## Architecture

<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Foresight Workshop</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
        <script src="../js/bootstrap.min.js"></script>
        <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
            </div>
            <div id="navigation">
                <p></p>
            </div>
        </div>

        <div id="content">
            <h1>Foresight Workshop</h1>

            <div class="info">

    

    
    
</div>

<h1 id="sam-bowman">Sam Bowman</h1>
<ul>
<li>we want LLMs to accurately answer factual answers that humans can’t</li>
<li>scalable oversight: human supervision is a source of trust</li>
<li>how do we supervise LLMs in domains where we don’t have full knowledge?</li>
</ul>
<p>have been working on this with debate w/evan hubinger, beth barnes etc. debate works by having two AI debaters and a human judge judging which argument is the best. the debaters have a skill advantage over the judge, and they’re trained entirely to be persuasive (reward proportional to judge’s confidence). evidence presentable in these arguments shouldbe verifiable. failure mode: hs/college debate where rhetoric takes all. however, it could also be like the civil trial system where hopefully it could truth-seek</p>
<p>the benchmark of experienced NYU debaters where they were debater and judge performed better when the judge had to judge debate as opposed to the human just consults the other. this is replicated in LLMs. takeaway: <strong>the adversarial element of debate is good for truth-seeking</strong></p>
<ul>
<li>even as the <em>dishonest debater gets more effective, the overall system is better at truth-seeking, because of the inherent adversarial interaction</em></li>
<li>debate seems to incentivize AI systems to answer questions they couldn’t otherwise (adversarial mechanism design etc.)</li>
</ul>
<p>questions:</p>
<ul>
<li>have you done asymmetric experiments? where the truth-seeking debater is better than the adversarial one or other things?
<ul>
<li>they have. if you have very unbalanced skill levels, then it does in fact bias the judge. way to counteract this: default to 50/50 when the judge notices that the debate is going badly (too much rhetoric etc)</li>
</ul></li>
<li>What does the human consultancy look like?
<ul>
<li>one side could have an unlimited openended interaction with the judge. (maybe should read the paper). reasoning: judge is worse than adversarial debater at picking holes in arguments</li>
</ul></li>
</ul>
<h1 id="fazl-barez">Fazl Barez</h1>
<p><strong>Unlearning in Large Language Models</strong></p>
<p>unlearing might be good bc</p>
<ul>
<li>you can remove harmful responses</li>
<li>you can erase copyright protected data</li>
<li>reduce hallucinations</li>
<li>reduce hazardous capabilities</li>
</ul>
<p>unlearning is just RLHF but you want to prevent the model from being helpful in certain aspects. and you want to do this in a way where you don’t have to retrain the entire model from scratch</p>
<p>also want to retain ‘novice representation’</p>
<p>issue: llms relearn removed concepts—how?</p>
<p>method:</p>
<ul>
<li>finetune model on dataset for named entity recognition</li>
<li>prune neurons linked to a specific concept</li>
<li>retrain the model on the same dataset &amp; figure what out</li>
</ul>
<p>re-learning happens faster when concept-neurons are pruned vs. random neurons</p>
<p>pruned concepts when present are later layers get transferred to earlier layers</p>
<p>neurons that relearn pruned concepts exhibit polysemanticity</p>
<p>(isn’t this only possible because of superposition????)</p>
<p>how do we actually unlearn the concepts? maybe you should just identify the concept neurons during training and prune them once they’re above a certain threshold</p>
<p>hm this research seems kind of bad, but also seems good</p>
<h1 id="evan-miyazono">Evan Miyazono</h1>
<p><strong>formal scalable oversight</strong></p>
<p>reducing ai risk with lead-bullet solutions</p>
<p>“guaranteed safe ai”</p>
<ul>
<li>ai systems should use a separate, auditable world-model</li>
<li>users can create a specification for safety in terms of that model</li>
<li>a verifier validates proposals against the specification using the world-model</li>
</ul>
<p>agi alignment focuses on individual preferences, not institutional or community preferences???</p>
<p>don’t wholly encode optima—just enable specifying constraints</p>
<p>then make rules to set rules for this</p>
<p>how do we take codebases and formally verify them with ai</p>
<p>can you predict bioactivity &amp; toxicity?</p>
<p>can you predict generalization behavior??</p>
<p>longterm goal: international standards organization for verifiably governable AI</p>
<p>make it economically efficient for companies to make safe AI</p>
<h1 id="kevin-esvelt">Kevin Esvelt</h1>
<p>SecureDNA as a model for safe capability evals</p>
<p>no recording</p>
<p>we need to ensure that we don’t lose the game in building bioweapons</p>
<p>delay, detect, defend: preparing for a future where thousands can release new pandemics</p>
<p>scenario 1: “wildfire”—as transmissible as measles and 95% fatality. they exist in rabbits. they don’t exist in humans, but adversaries could make it</p>
<p>“stealth”—hiv infects everyone before people notice</p>
<p>securing civilization against catastrophic pamdenics</p>
<p>~30,000 individuals can assmble any influenza virus. all the genome sequences are public. can dna synthesis screening actually works. NO????? 36/38 providers gave them the 1918 influenza virus????? with really really basic attacks</p>
<p>screen the dna without learning what’s in the orders</p>
<p>OH FUCK LLMS CAN ENCRYPT STUFF AND BREAK THIS OH FUCK LLMS ARE GOOD AT JAILBREAKING</p>
<p>what if you develop something outside the database???? llms will be able to do this????? what the actual fuck you could probably use llms to analyze the gene sequences such that they can just send the orders what the fuck we’re fucked</p>
<p>the worst thing to do at the moment is to lay out a correct strategy for building a wildfire agent. the risk is disclosing how to do this. biologists don’t have security mindset. this is an information problem. SecureDNA has curators. contact the curators and they can add it to the database.</p>
<p>dna for quick feasiblity test requires permission. make stuff annoying. make dangerous stuff annoying.</p>
<p>how do we eval these models? transcripts can’t be disclosed. but jesus christ we’re fucked</p>
<ul>
<li>does this depend on dna synthesis being centralized?
<ul>
<li>we’re all fucked we’re all fucked holy fuck what the actual fuck</li>
</ul></li>
</ul>
<p>malice analysis</p>
<h1 id="mimee-xu">Mimee Xu</h1>
<p>Neartermist safety</p>
<p>oversight measurement is stuck in an incentives rut</p>
<p>pay more attention to data rights</p>
<h1 id="esben-kran">Esben Kran</h1>
<p>multipolar agi security</p>
<p>multi-polar Ai is a near-term security question</p>
<p>unleashing sleeper agents</p>
<h1 id="anthony-aguirre">Anthony Aguirre</h1>
<p>it’s 2034, humanity is still empowered—how?</p>
<p>software can proliferate</p>
<p>compute governance is hopefully useful</p>
<p>tight supply chain</p>
<p>compute security is an opportunity (this is how we do this w/nuclear—enriched uranium)</p>
<p>h100s are wayyyy harder to build than enriched uranium. amd can’t build an h100</p>
<p>governance: contracts/regulations/agreements re: ai-specialized compute</p>
<p>security: hardware + software cryptographic based security measures are a crucial tool that could help verify and enforce these</p>
<p>governance:</p>
<ul>
<li>export controls</li>
<li>reporting requirements keep track of large clusters</li>
</ul>
<p>security side:</p>
<ul>
<li>gpu secure-boot firmware that requires:
<ul>
<li>permission to do flops</li>
<li>permissions to connect w/other gpus</li>
</ul></li>
<li>permissions are granted by signed messages from external custodians</li>
<li>message rtt times approximately geolocate gpus</li>
<li>permission to run/connect withheld from gpus not where they should be</li>
</ul>
<p>h100s already have shutoff switches. maybe if AIs don’t have shutoff switches then gpus have shutoff switches</p>
<p>example:</p>
<ul>
<li>off switch</li>
<li>compute cap</li>
</ul>
<p>vision:</p>
<ul>
<li>core safety limits are low-level and generally out of sight</li>
<li>technical side: demos/production software (fli, mithril)</li>
<li>field-building</li>
<li>policy development</li>
<li>policy engagement</li>
</ul>
<p>hiring to be a technical program manager in this space</p>
<p>fli-mithril proof of concept demo</p>
<p>rumors of factories in china taking gamer cards and moving the chips</p>
<h1 id="richard-ngo">Richard Ngo</h1>
<p>what should alignment aim for?</p>
<p>drawing on two frameworks</p>
<p>critch’s taxonomy of types of alignment:</p>
<ul>
<li>single-single alignment
<ul>
<li>longest standing area</li>
<li>should ai’s be aligned with humans instructions? intentions? preferences? values? extrapolated values? core problem: balancing obedience and paternalism</li>
<li>more principled way to think about this: defining empowerment. optimal policies tend to seek power (turner et al)</li>
</ul></li>
<li>single-multi alignment
<ul>
<li>many AIs working together: delegation + hierarchy</li>
<li>different kind of superintelligence: “superorganism”
<ul>
<li>copies of GPT-4 calling each other etc…</li>
</ul></li>
<li>highly parallel, more internal coordination problems (like corporation)</li>
<li>alignment could include
<ul>
<li>copies for monitoring</li>
<li>analyzing communications between copies</li>
<li>ai control</li>
</ul></li>
</ul></li>
<li>multi-single alignment
<ul>
<li>different ppl have conflicting goals and values</li>
<li>common criticism of alignment: “who to align to”</li>
<li>not good. this is acc a choice between different types of values
<ul>
<li>personal values</li>
<li>product values</li>
<li>political values</li>
<li>platform values</li>
<li>philosophical values</li>
</ul></li>
</ul></li>
<li>multi-multi alignment
<ul>
<li>unifyhing misuse and misalignment threat models
<ul>
<li>misalignment: ai gain illegitimate power by manipulating humans</li>
<li>misuse: humans gain illegitimate power by using ais</li>
</ul></li>
<li>coalition of humans + ais are actually very similar.</li>
<li>both would aim for state capture</li>
</ul></li>
</ul>
<p>respectively, each of these types of alignment should be considered as:</p>
<ul>
<li>empowerement</li>
<li>superorganism alignment</li>
<li>choosing high level values</li>
<li>preventing state capture</li>
</ul>
<p>openai’s recent model spec describes desirable AI behavior</p>
<h1 id="phil">Phil</h1>
<p>wtaf sparc guy</p>
<p>lionheart is the first vc fund focused on ai safety</p>
<p>developing counterfactual evaluation:</p>
<p>impact moic = impact value / capital invested (using joe carlsmith’s model)</p>
<p>how do multipolar scenarios get exacerbated by AI?</p>
<p>fathom radiant</p>
<p>encultured</p>
<h1 id="josh">Josh</h1>
<p>public AI</p>
<p>like healthcare.gov, BBC, library,</p>
<h1 id="keri-warr">Keri Warr</h1>
<p>anthropic (security engineer)</p>
<p>securing llm weights</p>
<p>things to do</p>
<p>rate limiting egress proxy:</p>
<ul>
<li>build a boundary around the cluster</li>
<li>limit bandwidth across that boundary to tens or hundreds of GB/day</li>
</ul>
<p>rate limits mean that exfiltration will take a TON of time</p>
<p>threat modeling:</p>
<ul>
<li>leaked credentials</li>
<li>attacker-triggered restarts</li>
<li>single point of failure</li>
</ul>
<p>we have a lot to gain by collaborating on cybersecurity
you can help to devise new methods
anthropic is hiring :D</p>
<h1 id="lawyer-person">Lawyer person</h1>
<p>google news can only exist because of hard-fought copyright relaxations—if we copyright AI in the way that AI safety measures would imply then we’re kind of fucked. Criminal liability for writing code is NOT the way to do this. Code sis speech, etc.</p>
<p>but then how can we hold labs accountable?</p>
<p>ca bill 1047 isn’t actually that bad. dk what Evan is going on about.</p>
<p>ok i want to work for evan</p>
<p>jeffrey: interesting test case of biological weapons. won’t be long before you actually have a model that can help you with dna synthesis.</p>
<p>“balance”</p>
<h1 id="jason-clinton">Jason Clinton</h1>
<p>securing AI at the frontier—anthropic CISO</p>
<p>what is practically useful in preventing nation-states from stealing models?</p>
<p>RSPs circumscribe everything that Anthropic does. “so much of how we think about the way we run the company runs through the RSPs”. Anthropic is a public benefit corporation, so then they put these priorities in a matrix and then solve.
anthropic will probably get to asl-3 soon. they’re going to leapfrog openai hm. so much of the way that internal comms works is through RSPs. it’s a very powerful governance tool. it’s effective self-governance.</p>
<p>nation state defense</p>
<p>anthropic is a nation-state target. they have been attacked at least once by a nation state attacker. largest tech companies are pretty good at nation state defense. so anthropic wants to copy paste that here.</p>
<p>but if a nation state turned its entire attention on a lab, they would be able to exfiltrate the weights. however, they are at asl-2, so this is fine.</p>
<p>ai deployment security</p>
<p>in the training environment: drawing boundaries between your customers &amp; teams etc. means that at the cluster level you can isolate stuff and then attackers can’t use it to leapfrog into the entire environment.</p>
<p>segmentation of clusters is big here etc.</p>
<p>in the inference environment: the deployment env needs to scale far beyond training env. confidential compute!! so many effective nation-state attacks use insider attacks. family member threatened, financial incentives, etc. this happened a couple months ago when someone was arrested going through san jose airport after stealing stuff from google.</p>
<p>defense: peer review, in the place where model weights are stored, even if you are an insider than you cannot steal it. if you’re running on google/amazon then someone in that environment can in fact steal the weights.</p>
<p>aws uses nitro. guarantees that an employee will not compromise aws. nitro is a hardware guarantee that this isn’t true. learn how nitro is structured &amp; use this for compute governance.</p>
<p>even if we solve alignment, we always need to draw a perimeter around model interactions that acts as a barrier. we need an input classifier and an output classifier. amazon has a product called e.g. guardrails.</p>
<p>hosted vs open weights. deployment needs guardrails. the open weights don’t matter, but the deployment guardrails need to exist and everyone needs to use.</p>
<p>cloud providers &gt;&gt; anthropic security team. they have to rely on cloud service providers. at amazon. ~43 on anthropic security team, 4 1yr ago</p>
<p>sub-agents architecture: air-gaps 5yrs out. models get self-exfiltration capacity at 5yrs out</p>
<h1 id="jeffrey-ladish">Jeffrey Ladish</h1>
<p>many deepfakes</p>
<p>jeffrey is very very good at marketing</p>
<p>automation enables scale</p>
<p>they were able to clone 80 trusted voices</p>
<p>automation of ai deception</p>
<h1 id="dean-tribble">Dean Tribble</h1>
<p>smart contract: contract-like arrangement, expressed in code, where the behavior of the program enforces the terms of the contract</p>
<p>inventor of the first smart contract</p>
<p>what does blockchain do for smart contract?</p>
<p>multiple independent computers under different admins &amp; jurisdictions come to consensus about data, order, and results. then this causes computational integrity to exist.</p>
<p>safe cooperation + ai</p>
<p>conventional software has type safety, legal contracts have none lol</p>
<p>smart contracts: either gets money back or the thing you want</p>
<p>conventional ai - dumb (lol problem is GANs???)</p>
<p>where you want is superintelligences using smart contracts to get assurance that the superintelligences are actually playing by the rules of the game</p>
<ul>
<li>ai economy</li>
</ul>
<h1 id="portia-murray">portia murray</h1>
<p>wargaming possible tai futures</p>
<p>simulations</p>
<ul>
<li>clearly specificed ontologies &amp; causality</li>
<li>let us explore emergent behaviors</li>
</ul>
<p>ai objectives institute is building understanding of the risks inherent to the wide-spread use of AI in new domains</p>
<p>hg wells developed the first recreational game for sale</p>
<p>gamers have pushed simulation research forward</p>
<p>wargaming TAI is underutilized</p>
<p>“if it is smart, it is vulnerable. if it can count, it can hack.”</p>
<h1 id="steve-stone">steve stone</h1>
<p>will agi make data defensible?</p>
<p>no one knows stuff about data</p>
<p>single largest problem in the entire history of cybersecurity is that no one has a good handle on how much data is under their jurisdiction</p>
<p>typical org has ~240 backend TB, which is a multi-petabyte</p>
<p>grows about 48% every 14 months</p>
<p>a typical orgs data will grow by 7x in the next five years</p>
<p>nothing grows like that</p>
<p>typical org in 10 yrs looks like the top 1% of the organizations today</p>
<p>your defensive boundary isn’t your infra, it’s your data</p>
<p>in sensitive data, the rate of increase is even faster than this</p>
<p>typical org today has 30m sensitive data records, in 5yrs it’ll be 150m</p>
<p>united health testimony—10 weeks for the date after that testimony</p>
<p>two people here have given testimony to senate &amp; house for congress huh. wild</p>
<h1 id="dimitri-usynin">dimitri usynin</h1>
<p>data privacy and governance regulations prohibit direct sensitive data sharing</p>
<p>also this person is going to go do ML at microsoft research. wild</p>
<p>federated learning (everyone trains their own model), swarm learning</p>
<p>k-anonymity doesn’t work</p>
<p>dp-sgd</p>
<h1 id="divya-siddarth">divya siddarth</h1>
<p>collective intelligence project</p>
<p>in a good world, people have agency and choice. concentration of power is bad for this most of the time.</p>
<p>more experiments combining collective intelligence and ai. isn’t this what openai was trying to elicit with their grants program?</p>
<p>collective constitutional ai</p>
<p>collective stuff works when:</p>
<ul>
<li>you’re trying to elicit tacit info
<ul>
<li>this is why markets work</li>
</ul></li>
<li>incorporating normative values</li>
<li>accelerating inherently with ai</li>
</ul>
<p>taiwan experiments worked well bc people were supposed to have opinions</p>
<p>w/openai it doesn’t rlly work so well bc they don’t have opinions</p>
<h1 id="bogdan">bogdan</h1>
<h1 id="people-to-talk-to">people to talk to</h1>
<ul>
<li>evan miyazono – working at atlas computing</li>
<li>jason anthropic – working on anthropic security</li>
<li>keri (other anthropic security) – more specific stuff</li>
<li>xiaohu zhu – provably safe agi, what are his takes</li>
<li>anthony aguirre – probably work on his team?</li>
<li>richard ngo – philosophy and fiction tbh?</li>
</ul>

        </div>
        <div id="footer">
            <p></p>
        </div>
    </body>
</html>

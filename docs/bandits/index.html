<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Bandits</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
        <script src="../js/bootstrap.min.js"></script>
        <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
            </div>
            <div id="navigation">
                <p></p>
            </div>
        </div>

        <div id="content">
            <h1>Bandits</h1>

            <div class="info">

    

    
    
</div>

<p>[<a href="../notes"><em>Notes</em></a>]</p>
<h2 id="multi-armed-bandits">Multi-Armed Bandits</h2>
<p>Looks at the explore-exploit problem. An agent is tasked with choosing from one of a set of lotteries which they only have limited information about at any given point. Each of the lotteries is part of the environment, &amp; has some probability distribution over reward the agent gets by sampling from the lottery.</p>
<p>Then the question becomes: what’s the best policy to implement for this agent? Random sampling is dumb, so it should be a decent benchmark. A cheater knows the rewards beforehand, and can just sample from the lottery with the highest expected payoff. But what sane learning strategy can we implement that converges to the optimal policy over time?</p>
<p>Naively, let’s say you implement an algorithm that just samples from the (historically, to the agent) highest-value lottery/arm. Let’s define some functions:</p>
<p><span class="math display">\[Q_t(a) = \frac{\text{sum of rewards when } a \text{ was taken prior to } t}{\text{# of times } a \text{ was taken prior to } t} = \frac{\sum_{i=1}^{t-1} R_i \cdot \unicode{x1D7D9}_{A_i=a}}{\sum_{i=1}^{t-1} \unicode{x1D7D9} _{A_i = a}}\]</span></p>
<p>This tracks the mean reward of taking the action <span class="math inline">\(a\)</span>, which corresponds to sampling from the arm <span class="math inline">\(A_i.\)</span> By the law of large numbers, this eventually converges to the true expected value of any <em>given</em> arm—but this does not guarantee that this algorithm converges to the <em>maximal</em> arm. For instance, the greedy strategy</p>
<p><span class="math display">\[A_t = \text{argmax}_a Q_t(a)\]</span></p>
<p><em>never explores</em>, so (depending on initialization) nearly always exploits the first arm it comes across. We can add an <span class="math inline">\(\epsilon\)</span>-randomness component to this, which ensures it acts greedily <em>most</em> of the time. And, in fact, in the limit this converges to the optimal policy, because each arm will be sampled an infinite amount of times and <span class="math inline">\(Q_t(a)\)</span> for each arm becomes accurate in the limit. Still has shit asymptotics.</p>
<p>We’d like stronger explore incentives that make achieving optimality more tractable (in shorter amounts of time). One way of doing this is the following:</p>
<p><span class="math display">\[A_t = \text{argmax}_a \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}\right]\]</span></p>
<p>where <span class="math inline">\(N_t(a)\)</span> is the number of times each arm has been sampled. This algorithm goes through all the arms at least once (dividing by 0 = inf), and then slowly converges on the one with the best reward. <span class="math inline">\(c\)</span> is some parameter. This <strong>upper confidence bound</strong> action selection converges more nicely. We take the square root because it’s approximately meant to capture the ‘variance’ of <span class="math inline">\(a\)</span>’s value, ergo the ‘upper bound’ nature of this optimization.</p>
<p>There’s some interesting infra-Bayesian stuff here.</p>
<p>Readings:</p>
<ul>
<li><a href="https://arxiv.org/abs/2405.05673">Imprecise Multi-Armed Bandits</a>—Vanessa Kosoy</li>
<li>Sutton &amp; Barto, Chapter 2</li>
<li></li>
</ul>

        </div>
        <div id="footer">
            <p></p>
        </div>
    </body>
</html>

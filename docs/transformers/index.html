<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Transformers</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
        <script src="../js/bootstrap.min.js"></script>
        <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
            </div>
            <div id="navigation">
                <p></p>
            </div>
        </div>

        <div id="content">
            <h1>Transformers</h1>

            <div class="info">

    

    
    
</div>

<p>To put somewhere: intuitions for <strong>activation space</strong> should be similar to those for phase space. GPT-2-small can be considered to have 12 x 768 neurons (check this!!), and each of these neurons is an element of this ‘activation space’. Perhaps it’s also good to completely reconstruct an NN architecture.</p>
<h2 id="architecture">Architecture</h2>

        </div>
        <div id="footer">
            <p></p>
        </div>
    </body>
</html>
